\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Deep Natural Language Processing Project\\Classification of Scientific Articles using SPECTER Embedding Model}

\author{\IEEEauthorblockN{Thomas Avare}
\IEEEauthorblockA{\textit{Master of Data Science, PoliTo} \\
\textit{exchange student at Politecnico di Torino from Grenbole INP - ENSIMAG}\\
Torino, Italy/Grenoble, France \\
thomas.avare@grenoble-inp.fr}}
\maketitle

\begin{abstract}
SPECTER, standing for \textit{Scientific Paper Embeddings using Citation-informed TransformERs} is a process to generate document-level embedding of scientific documents. SPECTER is based on pretrained Transformer language model and incorporating the citation graph to improve documents relatedness. SPECTER was benchmarked using SciDocs, an additional contribution to SPECTER, along previous scientific embedding models. SciDocs is an evaluation benchmark consisting of 7 document-level tasks such as citation prediction, document classification and recommendation. 

The goal of this project is to use SPECTER as a classification tool. The first part consists of understanding the principle of document-level embedding with SPECTER and its specificity, using SPECTER and SciDocs to reproduce the results presented in the original paper, focus on the text classification part being the subject of this project and some extension(s). 

In the approach of the development of SPECTER, The main idea was to use the citation-graph to introduce document relatedness into a triplet loss with  the query paper, a cited paper and a non cited paper as inputs and thus in the embedding. An idea for and extension for this project was to modify the loss to also include the date difference between the query paper and the positive paper to add/modify the information regarding the document relatedness.
\end{abstract}

\section{Introduction to SPECTER}

SPECTER (Cohan et al., 2020 \cite{b1})is an attempt to improve the embedding of sentences or whole documents by the mean of pre-trained language models to learn embeddings for scientific documents.The main idea was to incorporate inter-document relatedness into the Transformer language models such that the embedding could be more effective various tasks without any task-specific training for the embedding model. Transformer language models already exists, such as BERT, none of them used any inter-document context/information during their training. SPECTER uses the citation-graph to add inter-document context/information and these informations are used during the training of the model by the introduction of a triplet margin loss with a query paper $\mathcal P^Q$, a positive paper $\mathcal P^+$ which corresponds to a paper cited by the query paper and a negative paper $\mathcal P^-$ which corresponds to a non-cited paper by the query paper. So the triplet margin loss looks like: $$\mathcal L = \max\left\{d\left(\mathcal P^Q, \mathcal P^+\right) - d\left(\mathcal P^Q, \mathcal P^-\right) + m, 0\right\}$$ where $d$ is a distance between two embeddings, m a loss margin hyperparameter.

There are also different type of negative cases to a query paper. The "easy negatives" which are random papers not cited by the query paper and "hard negatives" which are cited paper by a cited paper but not by the query paper ($\mathcal P^1 \xrightarrow[]{cite} \mathcal P^2 \xrightarrow[]{cite}\mathcal P^3$ but $\mathcal P^1\not\xrightarrow[]{cite} \mathcal P^3$). This distinction was made provide more nuance in the training. 

They also used pretrained transformers network as a foundation for SPECTER, specifically the SciBERT (Beltagy et al., 2019\cite{b3}). After the training, it ended up becoming the state of the art model in embedding model used for various task (see next part for an introduction to SciDocs evaluation suite.

It's interesting to note that the idea of the citation graph was reused, the main idea was to transform the citation graph into a discrete space and pick the positives, easy negatives and hard negatives according to their distance in that space to the query paper and this model has become the new state of the art embedding model outperforming SPECTER (SciNCL, Ostendorff et al. 2022\cite{b2}).

\section{Introduction to Scidocs}

SciDocs (Cohan et al., 2020\cite{b1})is an evaluation framework introduce to compare different embedding models on different tasks. These tasks are Document classification, Citation prediction, user activity and Recommendation. The document classification is divided in two different type: MeSh classification, standing fro \textit{Medical Subject Headings}, and the paper topic classification known as MAG, standing for \textit{Microsoft Academic Graph}. The MeSH dataset consists of 23K academic medical papers distributed in 11 classes and the MAG consists of 25K papers distributed in 19 classes.
Since we're not interested in other tasks, we're not going to talk more about them.

\section{Goal of this Project}

The goal of this project is to use SPECTER as a tool for scientific document classification, so for MAG and MeSH classification. Another goal of this project was to introduce an extension to the initial project, I will talk about my idea in a forthcoming section, unfortunately I wasn't able to complete it but i will still briefly talk about it.

\section{The Classification Task}

As said before, SPECTER is an embedding model made to be versatile so it doesn't need any task-specific training.

\subsection{Importing SPECTER}

The first step to our classification journey was to import SPECTER. To do so, I simply cloned the official Github repository. After following the installation instructions. I had some versioning issues and one of my library wasn't downloading in the right place so I had to solve these issues. Afterwards, I was finally able to compute my first embeddings. Unfortunately, my computer doesn't have any GPU when I first started working and I decided to switch to a Google Colab notebook to pursue my experiments.

Before training my first models, I imported the SPECTER model accessible through the huggingFace transformer's library, then I imported the data provided by SiDocs. This contains all the data used for the SciDocs benchmarking, including the information of all papers (paper id, author, title, abstract, date of publication, citations and cited by), their classification (MAG and MeSH) and their embeddings. To import and manage all these informations, I used Pandas DataFrames which is really easy and convenient for these kind of tasks.

\subsection{Importing the Data}

After my first experiments to train models, I realized that it was not working for a very simple reason. The embeddings provided with SciDocs are those computed with the SPECTER model from the SPECTER GitHub and the SPECTER model from the HuggingFace Transformer library computes different embeddings for the same title and abstract. So I re-computed the embeddings used for the MAG and MeSH classifications and saved it inn a json file, for every paper, we have the paper id, The paper id, the title, the abstract and its embedding. To prepare the data for the classification, we simply have to import these information into a data frame and import the classification and merge it according to their paper id. The classification is already split into a train/test dataset for reproducibility purposes for the SciDocs benchmarking.

\subsection{Training Models}

After correctly importing the data and solving our embedding issues. Now we can train models for our classification task. I trained multiple types of models and will compare their performances according to their f1 score on the test dataset, just like they did with SciDocs so we can compare our models with the ones from the SciDocs benchmarking. Unfortunately, even by mimicking the training routine used for the SciDocs, I was never able to level or top it.

\begin{table}[!h]
\centering
\begin{tabular}{|l|l|l|}
\hline
                          & MAG   & MeSH  \\ \hline
Linear SVC                & 74.71 & 84.45 \\ \hline
SVC                       & 74.32 & 82.89 \\ \hline
Gaussian naive bayes      & 69.78 & 68.19 \\ \hline
K-neighbors               & 69.25 & 82.87 \\ \hline
SGD                       & 69.93 & 80.40 \\ \hline
CNN                       & 72.05 & 81.00 \\ \hline
SPECTER SciDocs benchmark & 81.95 & 86.44 \\ \hline
\end{tabular}
\end{table}

To try to top the models for the SciDocs, we tried to mimic the routine, at least on linearSVC and SVC, which is a parameter search over the C parameter, a regularization parameter. For the other models, we also did this parameter search search, mostly experimenting with various loss functions.

Unfortunately, the model with the best results is the linear SVC didn't always converge during the parameter search and it took too long when we increased the maximum number of iterations. Also when training a model with the same parameters as the one found with the parameter search, the results weren't improved.

For the CNN, I empirically tried various architectures.



\begin{thebibliography}{00}
\bibitem{b1}Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 

2020. SPECTER: Document-level Representation Learning using Citation-informed Transformers

\bibitem{b2} Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, Georg Rehm.

2022. Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings
\bibitem{b3} Iz Beltagy, Kyle Lo, and Arman Cohan.

2019. SciB- ERT: A Pretrained Language Model for Scientific Text. In EMNLP.

\end{thebibliography}
\vspace{12pt}

\end{document}

